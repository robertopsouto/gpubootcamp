{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robertopsouto/gpubootcamp/blob/colab/hpc_ai/PINN/English/python/jupyter_notebook/diffusion_1d/Diffusion_Problem_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install modulus"
      ],
      "metadata": {
        "id": "8lKW4UQcu_pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install nvidia-modulus nvidia-modulus-launch nvidia-modulus-sym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lsdjISru-Pd",
        "outputId": "216892e6-6925-4dc3-bf9f-57a540b46903"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-modulus\n",
            "  Downloading nvidia_modulus-0.3.0-py3-none-any.whl (290 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 290.9/290.9 kB 5.6 MB/s eta 0:00:00\n",
            "Collecting nvidia-modulus-launch\n",
            "  Downloading nvidia_modulus.launch-0.3.0-py3-none-any.whl (23 kB)\n",
            "Collecting nvidia-modulus-sym\n",
            "  Downloading nvidia_modulus.sym-1.2.0-py3-none-any.whl (305 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 305.3/305.3 kB 11.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-modulus) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy<1.25,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from nvidia-modulus) (1.23.5)\n",
            "Requirement already satisfied: xarray>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-modulus) (2023.7.0)\n",
            "Collecting zarr>=2.14.2 (from nvidia-modulus)\n",
            "  Downloading zarr-2.16.1-py3-none-any.whl (206 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 206.9/206.9 kB 11.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: fsspec>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-modulus) (2023.6.0)\n",
            "Collecting s3fs>=2023.5.0 (from nvidia-modulus)\n",
            "  Downloading s3fs-2023.9.2-py3-none-any.whl (28 kB)\n",
            "Collecting nvidia-dali-cuda110>=1.16.0 (from nvidia-modulus)\n",
            "  Downloading nvidia_dali_cuda110-1.30.0-9783405-py3-none-manylinux2014_x86_64.whl (493.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 493.6/493.6 MB 2.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: setuptools>=67.6.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-modulus) (67.7.2)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from nvidia-modulus) (2023.7.22)\n",
            "Collecting hydra-core>=1.2.0 (from nvidia-modulus-launch)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 154.5/154.5 kB 21.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: termcolor>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from nvidia-modulus-launch) (2.3.0)\n",
            "Collecting wandb>=0.13.7 (from nvidia-modulus-launch)\n",
            "  Downloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 94.4 MB/s eta 0:00:00\n",
            "Collecting mlflow>=2.1.1 (from nvidia-modulus-launch)\n",
            "  Downloading mlflow-2.7.1-py3-none-any.whl (18.5 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.5/18.5 MB 91.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pytest>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-modulus-launch) (7.4.2)\n",
            "Requirement already satisfied: pydantic>=1.10.2 in /usr/local/lib/python3.10/dist-packages (from nvidia-modulus-launch) (1.10.13)\n",
            "Requirement already satisfied: imageio>=2.28.1 in /usr/local/lib/python3.10/dist-packages (from nvidia-modulus-launch) (2.31.5)\n",
            "Requirement already satisfied: moviepy>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from nvidia-modulus-launch) (1.0.3)\n",
            "Requirement already satisfied: tqdm>=4.60.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-modulus-launch) (4.66.1)\n",
            "Collecting chaospy>=4.3.7 (from nvidia-modulus-sym)\n",
            "  Downloading chaospy-4.3.13-py3-none-any.whl (254 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 254.1/254.1 kB 32.5 MB/s eta 0:00:00\n",
            "Collecting Cython==0.29.28 (from nvidia-modulus-sym)\n",
            "  Downloading Cython-0.29.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/1.9 MB 63.8 MB/s eta 0:00:00\n",
            "Collecting numpy-stl==2.16.3 (from nvidia-modulus-sym)\n",
            "  Downloading numpy-stl-2.16.3.tar.gz (772 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 772.1/772.1 kB 64.1 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting opencv-python==4.5.5.64 (from nvidia-modulus-sym)\n",
            "  Downloading opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.5 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.5/60.5 MB 10.2 MB/s eta 0:00:00\n",
            "Collecting scikit-learn==1.0.2 (from nvidia-modulus-sym)\n",
            "  Downloading scikit_learn-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.5/26.5 MB 62.8 MB/s eta 0:00:00\n",
            "Collecting symengine>=0.10.0 (from nvidia-modulus-sym)\n",
            "  Downloading symengine-0.10.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (37.4 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 37.4/37.4 MB 12.9 MB/s eta 0:00:00\n",
            "Collecting sympy==1.5.1 (from nvidia-modulus-sym)\n",
            "  Downloading sympy-1.5.1-py2.py3-none-any.whl (5.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 75.6 MB/s eta 0:00:00\n",
            "Collecting timm==0.5.4 (from nvidia-modulus-sym)\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 431.5/431.5 kB 44.8 MB/s eta 0:00:00\n",
            "Collecting torch-optimizer==0.3.0 (from nvidia-modulus-sym)\n",
            "  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.9/61.9 kB 9.0 MB/s eta 0:00:00\n",
            "Collecting transforms3d==0.3.1 (from nvidia-modulus-sym)\n",
            "  Downloading transforms3d-0.3.1.tar.gz (62 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 9.1 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting typing==3.7.4.3 (from nvidia-modulus-sym)\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.6/78.6 kB 12.1 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting vtk>=9.2.6 (from nvidia-modulus-sym)\n",
            "  Downloading vtk-9.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (79.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.3/79.3 MB 9.3 MB/s eta 0:00:00\n",
            "Collecting pillow==9.3.0 (from nvidia-modulus-sym)\n",
            "  Downloading Pillow-9.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/3.3 MB 99.6 MB/s eta 0:00:00\n",
            "Collecting notebook==6.4.12 (from nvidia-modulus-sym)\n",
            "  Downloading notebook-6.4.12-py3-none-any.whl (9.9 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/9.9 MB 88.5 MB/s eta 0:00:00\n",
            "Collecting mistune==2.0.3 (from nvidia-modulus-sym)\n",
            "  Downloading mistune-2.0.3-py2.py3-none-any.whl (24 kB)\n",
            "Collecting pint==0.19.2 (from nvidia-modulus-sym)\n",
            "  Downloading Pint-0.19.2.tar.gz (292 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 292.0/292.0 kB 37.1 MB/s eta 0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: tensorboard>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-modulus-sym) (2.13.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook==6.4.12->nvidia-modulus-sym) (3.1.2)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from notebook==6.4.12->nvidia-modulus-sym) (6.3.2)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.10/dist-packages (from notebook==6.4.12->nvidia-modulus-sym) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook==6.4.12->nvidia-modulus-sym) (23.1.0)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.10/dist-packages (from notebook==6.4.12->nvidia-modulus-sym) (5.7.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook==6.4.12->nvidia-modulus-sym) (5.4.0)\n",
            "Requirement already satisfied: jupyter-client>=5.3.4 in /usr/local/lib/python3.10/dist-packages (from notebook==6.4.12->nvidia-modulus-sym) (6.1.12)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from notebook==6.4.12->nvidia-modulus-sym) (0.2.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook==6.4.12->nvidia-modulus-sym) (5.9.2)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook==6.4.12->nvidia-modulus-sym) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook==6.4.12->nvidia-modulus-sym) (1.5.8)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from notebook==6.4.12->nvidia-modulus-sym) (5.5.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook==6.4.12->nvidia-modulus-sym) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook==6.4.12->nvidia-modulus-sym) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook==6.4.12->nvidia-modulus-sym) (0.17.1)\n",
            "Requirement already satisfied: python-utils>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from numpy-stl==2.16.3->nvidia-modulus-sym) (3.8.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2->nvidia-modulus-sym) (1.11.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2->nvidia-modulus-sym) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2->nvidia-modulus-sym) (3.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy==1.5.1->nvidia-modulus-sym) (1.3.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.5.4->nvidia-modulus-sym) (0.15.2+cu118)\n",
            "Collecting pytorch-ranger>=0.1.1 (from torch-optimizer==0.3.0->nvidia-modulus-sym)\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Collecting numpoly>=1.2.5 (from chaospy>=4.3.7->nvidia-modulus-sym)\n",
            "  Downloading numpoly-1.2.11-py3-none-any.whl (148 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 148.1/148.1 kB 20.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from chaospy>=4.3.7->nvidia-modulus-sym) (6.8.0)\n",
            "Collecting omegaconf<2.4,>=2.2 (from hydra-core>=1.2.0->nvidia-modulus-launch)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.5/79.5 kB 11.1 MB/s eta 0:00:00\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.2.0->nvidia-modulus-launch)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.0/117.0 kB 15.9 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.2.0->nvidia-modulus-launch) (23.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.1.1->nvidia-modulus-launch) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle<3 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.1.1->nvidia-modulus-launch) (2.2.1)\n",
            "Collecting databricks-cli<1,>=0.8.7 (from mlflow>=2.1.1->nvidia-modulus-launch)\n",
            "  Downloading databricks_cli-0.18.0-py2.py3-none-any.whl (150 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 150.3/150.3 kB 18.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.1.1->nvidia-modulus-launch) (0.4)\n",
            "Collecting gitpython<4,>=2.1.0 (from mlflow>=2.1.1->nvidia-modulus-launch)\n",
            "  Downloading GitPython-3.1.37-py3-none-any.whl (190 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 190.0/190.0 kB 29.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.1.1->nvidia-modulus-launch) (6.0.1)\n",
            "Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.1.1->nvidia-modulus-launch) (3.20.3)\n",
            "Requirement already satisfied: pytz<2024 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.1.1->nvidia-modulus-launch) (2023.3.post1)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.1.1->nvidia-modulus-launch) (2.31.0)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.1.1->nvidia-modulus-launch) (0.4.4)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow>=2.1.1->nvidia-modulus-launch)\n",
            "  Downloading alembic-1.12.0-py3-none-any.whl (226 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 226.0/226.0 kB 26.9 MB/s eta 0:00:00\n",
            "Collecting docker<7,>=4.0.0 (from mlflow>=2.1.1->nvidia-modulus-launch)\n",
            "  Downloading docker-6.1.3-py3-none-any.whl (148 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 148.1/148.1 kB 22.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: Flask<3 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.1.1->nvidia-modulus-launch) (2.2.5)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.1.1->nvidia-modulus-launch) (1.5.3)\n",
            "Collecting querystring-parser<2 (from mlflow>=2.1.1->nvidia-modulus-launch)\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.1.1->nvidia-modulus-launch) (2.0.21)\n",
            "Requirement already satisfied: pyarrow<14,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.1.1->nvidia-modulus-launch) (9.0.0)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.1.1->nvidia-modulus-launch) (3.5)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.1.1->nvidia-modulus-launch) (3.7.1)\n",
            "Collecting gunicorn<22 (from mlflow>=2.1.1->nvidia-modulus-launch)\n",
            "  Downloading gunicorn-21.2.0-py3-none-any.whl (80 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.2/80.2 kB 11.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.3->nvidia-modulus-launch) (4.4.2)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.3->nvidia-modulus-launch) (0.1.10)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.3->nvidia-modulus-launch) (0.4.9)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda110>=1.16.0->nvidia-modulus) (1.6.3)\n",
            "Requirement already satisfied: gast>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda110>=1.16.0->nvidia-modulus) (0.4.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda110>=1.16.0->nvidia-modulus) (0.1.8)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.2->nvidia-modulus-launch) (4.5.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=6.0.0->nvidia-modulus-launch) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.0.0->nvidia-modulus-launch) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.0.0->nvidia-modulus-launch) (1.1.3)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.0.0->nvidia-modulus-launch) (2.0.1)\n",
            "Collecting aiobotocore~=2.5.4 (from s3fs>=2023.5.0->nvidia-modulus)\n",
            "  Downloading aiobotocore-2.5.4-py3-none-any.whl (73 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.4/73.4 kB 11.5 MB/s eta 0:00:00\n",
            "Collecting fsspec>=2023.1.0 (from nvidia-modulus)\n",
            "  Downloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173.4/173.4 kB 26.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from s3fs>=2023.5.0->nvidia-modulus) (3.8.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.8.0->nvidia-modulus-sym) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.8.0->nvidia-modulus-sym) (1.59.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.8.0->nvidia-modulus-sym) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.8.0->nvidia-modulus-sym) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.8.0->nvidia-modulus-sym) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.8.0->nvidia-modulus-sym) (3.0.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.8.0->nvidia-modulus-sym) (0.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->nvidia-modulus) (3.12.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->nvidia-modulus) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->nvidia-modulus) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=2.0.0->nvidia-modulus) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=2.0.0->nvidia-modulus) (17.0.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.7->nvidia-modulus-launch) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb>=0.13.7->nvidia-modulus-launch)\n",
            "  Downloading sentry_sdk-1.32.0-py2.py3-none-any.whl (240 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 241.0/241.0 kB 30.5 MB/s eta 0:00:00\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb>=0.13.7->nvidia-modulus-launch)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb>=0.13.7->nvidia-modulus-launch)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting setproctitle (from wandb>=0.13.7->nvidia-modulus-launch)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.7->nvidia-modulus-launch) (1.4.4)\n",
            "Collecting asciitree (from zarr>=2.14.2->nvidia-modulus)\n",
            "  Downloading asciitree-0.3.3.tar.gz (4.0 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting fasteners (from zarr>=2.14.2->nvidia-modulus)\n",
            "  Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Collecting numcodecs>=0.10.0 (from zarr>=2.14.2->nvidia-modulus)\n",
            "  Downloading numcodecs-0.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.7 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/7.7 MB 91.8 MB/s eta 0:00:00\n",
            "Collecting botocore<1.31.18,>=1.31.17 (from aiobotocore~=2.5.4->s3fs>=2023.5.0->nvidia-modulus)\n",
            "  Downloading botocore-1.31.17-py3-none-any.whl (11.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.1/11.1 MB 101.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore~=2.5.4->s3fs>=2023.5.0->nvidia-modulus) (1.15.0)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore~=2.5.4->s3fs>=2023.5.0->nvidia-modulus)\n",
            "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2023.5.0->nvidia-modulus) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2023.5.0->nvidia-modulus) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2023.5.0->nvidia-modulus) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2023.5.0->nvidia-modulus) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2023.5.0->nvidia-modulus) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2023.5.0->nvidia-modulus) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2023.5.0->nvidia-modulus) (1.3.1)\n",
            "Collecting Mako (from alembic!=1.10.0,<2->mlflow>=2.1.1->nvidia-modulus-launch)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.7/78.7 kB 11.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->nvidia-dali-cuda110>=1.16.0->nvidia-modulus) (1.16.0)\n",
            "Requirement already satisfied: pyjwt>=1.7.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow>=2.1.1->nvidia-modulus-launch) (2.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow>=2.1.1->nvidia-modulus-launch) (3.2.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow>=2.1.1->nvidia-modulus-launch) (0.9.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.7 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow>=2.1.1->nvidia-modulus-launch) (2.0.6)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker<7,>=4.0.0->mlflow>=2.1.1->nvidia-modulus-launch) (1.6.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3->mlflow>=2.1.1->nvidia-modulus-launch) (2.1.2)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=2.1.0->mlflow>=2.1.1->nvidia-modulus-launch)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 9.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.8.0->nvidia-modulus-sym) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.8.0->nvidia-modulus-sym) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.8.0->nvidia-modulus-sym) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.8.0->nvidia-modulus-sym) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->chaospy>=4.3.7->nvidia-modulus-sym) (3.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->notebook==6.4.12->nvidia-modulus-sym) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=5.3.4->notebook==6.4.12->nvidia-modulus-sym) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook==6.4.12->nvidia-modulus-sym) (3.11.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.1.1->nvidia-modulus-launch) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.1.1->nvidia-modulus-launch) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.1.1->nvidia-modulus-launch) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.1.1->nvidia-modulus-launch) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.1.1->nvidia-modulus-launch) (3.1.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.4.12->nvidia-modulus-sym) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.4.12->nvidia-modulus-sym) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.4.12->nvidia-modulus-sym) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.4.12->nvidia-modulus-sym) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.4.12->nvidia-modulus-sym) (0.2.2)\n",
            "INFO: pip is looking at multiple versions of nbconvert to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting nbconvert>=5 (from notebook==6.4.12->nvidia-modulus-sym)\n",
            "  Downloading nbconvert-7.9.2-py3-none-any.whl (256 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 256.4/256.4 kB 31.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.4.12->nvidia-modulus-sym) (0.8.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.4.12->nvidia-modulus-sym) (1.5.0)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.4.12->nvidia-modulus-sym) (2.16.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.4.12->nvidia-modulus-sym) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook==6.4.12->nvidia-modulus-sym) (2.18.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook==6.4.12->nvidia-modulus-sym) (4.19.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow>=2.1.1->nvidia-modulus-launch) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow>=2.1.1->nvidia-modulus-launch) (3.0.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook==6.4.12->nvidia-modulus-sym) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook==6.4.12->nvidia-modulus-sym) (21.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->notebook==6.4.12->nvidia-modulus-sym) (7.34.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook==6.4.12->nvidia-modulus-sym) (0.5.1)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.4->s3fs>=2023.5.0->nvidia-modulus)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting urllib3<3,>=1.26.7 (from databricks-cli<1,>=0.8.7->mlflow>=2.1.1->nvidia-modulus-launch)\n",
            "  Downloading urllib3-1.26.17-py2.py3-none-any.whl (143 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.4/143.4 kB 21.6 MB/s eta 0:00:00\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow>=2.1.1->nvidia-modulus-launch)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Collecting jedi>=0.16 (from ipython>=5.0.0->ipykernel->notebook==6.4.12->nvidia-modulus-sym)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 87.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook==6.4.12->nvidia-modulus-sym) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook==6.4.12->nvidia-modulus-sym) (3.0.39)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook==6.4.12->nvidia-modulus-sym) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook==6.4.12->nvidia-modulus-sym) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook==6.4.12->nvidia-modulus-sym) (4.8.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.4.12->nvidia-modulus-sym) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.4.12->nvidia-modulus-sym) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.4.12->nvidia-modulus-sym) (0.10.4)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.8.0->nvidia-modulus-sym) (0.5.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook==6.4.12->nvidia-modulus-sym) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook==6.4.12->nvidia-modulus-sym) (2.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook==6.4.12->nvidia-modulus-sym) (2.21)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->notebook==6.4.12->nvidia-modulus-sym) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->notebook==6.4.12->nvidia-modulus-sym) (0.2.8)\n",
            "Building wheels for collected packages: numpy-stl, pint, transforms3d, typing, antlr4-python3-runtime, asciitree, pathtools\n",
            "  Building wheel for numpy-stl (setup.py): started\n",
            "  Building wheel for numpy-stl (setup.py): finished with status 'done'\n",
            "  Created wheel for numpy-stl: filename=numpy_stl-2.16.3-cp310-cp310-linux_x86_64.whl size=174292 sha256=627577e77f84773c6d6da96076e57c1f142da8773be1466bb622234b212ca748\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/59/9f/28b045800f3ca6beca8d9805d68009f1b006bb7ad67bb3b892\n",
            "  Building wheel for pint (pyproject.toml): started\n",
            "  Building wheel for pint (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for pint: filename=Pint-0.19.2-py3-none-any.whl size=230998 sha256=ca2e4ddd0fb47e57771684cd63e4a47f0c134253064dfad0e73892e138c42064\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/00/87/c216aa83b91597845d9fec6fcd9dbe31c945a25d968dd4765a\n",
            "  Building wheel for transforms3d (setup.py): started\n",
            "  Building wheel for transforms3d (setup.py): finished with status 'done'\n",
            "  Created wheel for transforms3d: filename=transforms3d-0.3.1-py3-none-any.whl size=59356 sha256=d5f3b6cd5b512093b7af84f3f4fcd383ced5ec426f86e7cfe2f973b988c098cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/b6/70/b90dfb92fc3eab4cd4fe00a49f5f45cbcd203678ab6379e9d4\n",
            "  Building wheel for typing (setup.py): started\n",
            "  Building wheel for typing (setup.py): finished with status 'done'\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26305 sha256=91c433b3e0038d6b6005d8da6e59f0eef61969ad75f1088d45f0a067d9da03c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/d0/9e/1f26ebb66d9e1732e4098bc5a6c2d91f6c9a529838f0284890\n",
            "  Building wheel for antlr4-python3-runtime (setup.py): started\n",
            "  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=5b6dd87a5eacd44031ce0b5b58e8f58b7dd856e4dd64b233d0d6baef4ccd81c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for asciitree (setup.py): started\n",
            "  Building wheel for asciitree (setup.py): finished with status 'done'\n",
            "  Created wheel for asciitree: filename=asciitree-0.3.3-py3-none-any.whl size=5034 sha256=b8abc6c4ea69399db15fec8ab924f27293d32f12a6323c2307e6d0cc67a06c11\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/4e/be/1171b40f43b918087657ec57cf3b81fa1a2e027d8755baa184\n",
            "  Building wheel for pathtools (setup.py): started\n",
            "  Building wheel for pathtools (setup.py): finished with status 'done'\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=3a0b0a4589744dce502fe6f2fe1fbdcf788da84c88f0c769ee02a2dded5d1f66\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built numpy-stl pint transforms3d typing antlr4-python3-runtime asciitree pathtools\n",
            "Installing collected packages: transforms3d, pathtools, mistune, asciitree, antlr4-python3-runtime, urllib3, typing, sympy, symengine, smmap, setproctitle, querystring-parser, pint, pillow, opencv-python, omegaconf, numcodecs, Mako, jmespath, jedi, gunicorn, fsspec, fasteners, docker-pycreds, Cython, aioitertools, zarr, sentry-sdk, scikit-learn, nvidia-dali-cuda110, numpy-stl, numpoly, hydra-core, gitdb, botocore, alembic, vtk, gitpython, docker, databricks-cli, chaospy, aiobotocore, wandb, s3fs, mlflow, nbconvert, notebook, pytorch-ranger, torch-optimizer, timm, nvidia-modulus, nvidia-modulus-sym, nvidia-modulus-launch\n",
            "  Attempting uninstall: mistune\n",
            "    Found existing installation: mistune 0.8.4\n",
            "    Uninstalling mistune-0.8.4:\n",
            "      Successfully uninstalled mistune-0.8.4\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.6\n",
            "    Uninstalling urllib3-2.0.6:\n",
            "      Successfully uninstalled urllib3-2.0.6\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.12\n",
            "    Uninstalling sympy-1.12:\n",
            "      Successfully uninstalled sympy-1.12\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.8.0.76\n",
            "    Uninstalling opencv-python-4.8.0.76:\n",
            "      Successfully uninstalled opencv-python-4.8.0.76\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2023.6.0\n",
            "    Uninstalling fsspec-2023.6.0:\n",
            "      Successfully uninstalled fsspec-2023.6.0\n",
            "  Attempting uninstall: Cython\n",
            "    Found existing installation: Cython 3.0.3\n",
            "    Uninstalling Cython-3.0.3:\n",
            "      Successfully uninstalled Cython-3.0.3\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: nbconvert\n",
            "    Found existing installation: nbconvert 6.5.4\n",
            "    Uninstalling nbconvert-6.5.4:\n",
            "      Successfully uninstalled nbconvert-6.5.4\n",
            "  Attempting uninstall: notebook\n",
            "    Found existing installation: notebook 6.5.5\n",
            "    Uninstalling notebook-6.5.5:\n",
            "      Successfully uninstalled notebook-6.5.5\n",
            "Successfully installed Cython-0.29.28 Mako-1.2.4 aiobotocore-2.5.4 aioitertools-0.11.0 alembic-1.12.0 antlr4-python3-runtime-4.9.3 asciitree-0.3.3 botocore-1.31.17 chaospy-4.3.13 databricks-cli-0.18.0 docker-6.1.3 docker-pycreds-0.4.0 fasteners-0.19 fsspec-2023.9.2 gitdb-4.0.10 gitpython-3.1.37 gunicorn-21.2.0 hydra-core-1.3.2 jedi-0.19.1 jmespath-1.0.1 mistune-2.0.3 mlflow-2.7.1 nbconvert-7.9.2 notebook-6.4.12 numcodecs-0.12.0 numpoly-1.2.11 numpy-stl-2.16.3 nvidia-dali-cuda110-1.30.0 nvidia-modulus-0.3.0 nvidia-modulus-launch-0.3.0 nvidia-modulus-sym-1.2.0 omegaconf-2.3.0 opencv-python-4.5.5.64 pathtools-0.1.2 pillow-9.3.0 pint-0.19.2 pytorch-ranger-0.1.1 querystring-parser-1.2.4 s3fs-2023.9.2 scikit-learn-1.0.2 sentry-sdk-1.32.0 setproctitle-1.3.3 smmap-5.0.1 symengine-0.10.0 sympy-1.5.1 timm-0.5.4 torch-optimizer-0.3.0 transforms3d-0.3.1 typing-3.7.4.3 urllib3-1.26.17 vtk-9.2.6 wandb-0.15.12 zarr-2.16.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.9.2 which is incompatible.\n",
            "google-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.4.12 which is incompatible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjz36DEYuRHg"
      },
      "source": [
        "# Steady State 1D Diffusion in a Composite Bar using PINNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7oHwWVQuRHp"
      },
      "source": [
        "This notebook give you a headstart in solving your own Partial Differential Equations (PDEs) using neural networks. Let's quickly recap the theory of PINNs before proceeding. We will embed the physics of the problem in the the neural networks and in such setting, we can use them to approximate the solution to a given differential equation and boundary condition without any training data from other solvers. More specifically, the neural network will be trained to minimize a loss function which is formed using the differential equation and the boundary conditions. If the network is able to minimize this loss then it will in effect solve the given differential equation. More information about the Physics Informed Neural Networks (PINNs) can be found in the [paper](https://www.sciencedirect.com/science/article/pii/S0021999118307125?casa_token=1CnSbVeDwJ0AAAAA:-8a6ZMjO7RgYjFBAxIoVU2sWAdsqFzLRTNHA1BkRryNPXx5Vjc8hCDCkS99gcZR0B1rpa33a30yG) published by Raissi et al.\n",
        "\n",
        "In this notebook we will solve the steady 1 dimensional heat transfer in a composite bar. We will use NVIDIA's Modulus library to create the problem setup. You can refer to the [Modulus User Documentation](https://docs.nvidia.com/deeplearning/modulus/index.html) for more examples on solving different types of PDEs using the Modulus library, Modulus APIs, Release notes, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQadlBXQuRHr"
      },
      "source": [
        "### Learning Outcomes\n",
        "1. How to use Modulus to simulate physics problems using PINNs\n",
        "    1. How to write your own PDEs and formulate the different losses\n",
        "    2. How to use the Constructive Solid Geometry (CSG) module\n",
        "2. How to use Modulus to solve a parameterized PDEs\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk6N4spOuRHs"
      },
      "source": [
        "## Problem Description\n",
        "\n",
        "Our aim is to obtain the temperature distribution inside the bar that is made up of two materials with different thermal conductivity. The geometry and the problem specification of the problem can be seen below\n",
        "\n",
        "<img src=\"https://github.com/robertopsouto/gpubootcamp/blob/colab/hpc_ai/PINN/English/python/jupyter_notebook/diffusion_1d/diffusion_bar_geometry.png?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
        "\n",
        "The composite bar extends from $x=0$ to $x=2$. The bar has material of conductivity $D_1=10$ from $x=0$ to $x=1$ and $D_2=0.1$ from $x=1$ to $x=2$. Both the ends of the bar, $x=0$ and $x=2$ are maintained at a constant temperatures of $0$ and $100$ respectively. For simplicity of modeling, we will treat the composite bar as two separate bars, bar 1 and bar 2, whose ends are joined together. We will treat the temperatures in the bar 1 as $U_1$ and the temperature in bar 2 as $U_2$.\n",
        "\n",
        "The equations and boundary conditions governing the problem can be mathematically expressed as\n",
        "\n",
        "One dimensional diffusion of temperature in bar 1 and 2:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{d}{dx}\\left( D_1\\frac{dU_1}{dx} \\right) = 0, && \\text{when } 0<x<1 \\\\\n",
        "\\frac{d}{dx}\\left( D_2\\frac{dU_2}{dx} \\right) = 0, && \\text{when } 1<x<2 \\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Flux and temperature continuity at interface $(x=1)$\n",
        "$$\n",
        "\\begin{align}\n",
        "D_1\\frac{dU_1}{dx} = D_2\\frac{dU_2}{dx}, && \\text{when } x=1 \\\\\n",
        "U_1 = U_2, && \\text{when } x=1 \\\\\n",
        "\\end{align}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFaU6bfsuRHv"
      },
      "source": [
        "## Case Setup\n",
        "\n",
        "Now that we have our problem defined, let's take a look at the code required to solve it using Modulus. Modulus has a variety of helper functions/APIs that will help us to set up the problem with ease. It has APIs to model geometry in a parameterized fashion using the Constructive Solid Geometry (CSG) module, write-up the required equations in a user-friendly symbolic format and comes with several advanced neural network architectures to choose for more complicated problems.\n",
        "\n",
        "Now let's start the problem by importing the required libraries and packages\n",
        "\n",
        "### Note\n",
        "\n",
        "In this notebook we will describe the contents of the [`diffusion_bar.py`](../../source_code/diffusion_1d/diffusion_bar.py) script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_Fb6ZrVuRHw"
      },
      "source": [
        "```python\n",
        "import torch\n",
        "import numpy as np\n",
        "from sympy import Symbol, Eq, Function, Number\n",
        "\n",
        "import modulus\n",
        "from modulus.hydra import ModulusConfig, instantiate_arch\n",
        "from modulus.solver import Solver\n",
        "from modulus.domain import Domain\n",
        "from modulus.geometry.primitives_1d import Line1D\n",
        "from modulus.domain.constraint import (\n",
        "    PointwiseBoundaryConstraint,\n",
        "    PointwiseInteriorConstraint,\n",
        ")\n",
        "from modulus.domain.validator import PointwiseValidator\n",
        "from modulus.domain.monitor import PointwiseMonitor\n",
        "from modulus.key import Key\n",
        "from modulus.node import Node\n",
        "from modulus.eq.pde import PDE\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GILlsRq3uRHx"
      },
      "source": [
        "From v22.03 modulus supports APIs for solving data-driven problems where the data is available on grid/mesh as well as the PINN problems where point clouds are used for problem formulation. In this section, since we will be solving the 1d diffusion problem using the PINN approach, we will use the functions and classes from the `modulus.solver` module. The `Solver` class trains and evaluates the Modulus' neural network solver. The `Domain` class is used to add constraints, monitors, validators, etc. to the problem.\n",
        "\n",
        "The `geometry` module contains predefined geometries respectively that one can use to define the problem. We will describe each of them in detail as we move forward in the code. For more detailed information on all the different modules, we recommended you to refer the [Modulus API Documentation](https://docs.nvidia.com/deeplearning/modulus/api_source/api_index.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TswcMKTAuRHz"
      },
      "source": [
        "## Creating the geometry\n",
        "\n",
        "In this problem, we will create the 1-dimensional geometry using `Line1D` class from the geometry module. The module also contains several 2d and 3d shapes like rectangle, circle, triangle, cuboids, sphere, torus, cones, tetrahedrons, etc. We will define the one dimensional line object using the two end-points. For composite bar, we will create two separate bars as defined in the problem statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alO3qg3nuRH0"
      },
      "source": [
        "```python\n",
        "# params for domain\n",
        "L1 = Line1D(0,1)\n",
        "L2 = Line1D(1,2)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2NT8MYmuRH1"
      },
      "source": [
        "Next we will define the properties for the problem which will later be used while making the equations, boundary conditions, etc. Also, for this problem, we can find the temperature at the interface analytically and we will use that as to form validation data to compare our neural network results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAKk6TtuuRH2"
      },
      "source": [
        "```python\n",
        "D1 = 1e1\n",
        "D2 = 1e-1\n",
        "\n",
        "Tc = 100\n",
        "Ta = 0\n",
        "Tb = (Tc + (D1 / D2) * Ta) / (1 + (D1 / D2))\n",
        "\n",
        "print(Ta)\n",
        "print(Tb)\n",
        "print(Tc)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPsxZrOCuRH3"
      },
      "source": [
        "## Defining the differential equations for the problem\n",
        "\n",
        "The `PDE` class allows us to write the equations symbolically in Sympy. This allows you to quickly write your equations in the most natural way possible. The Sympy equations are converted to PyTorch expressions in the back-end and can also be printed to ensure correct implementation.\n",
        "\n",
        "Modulus also comes with several common PDEs predefined for the user to choose from. Some of the PDEs that are already available in the PDEs module are: Navier Stokes, Linear Elasticity, Advection Diffusion, Wave Equations, etc.\n",
        "\n",
        "Let's create the PDE to define the diffusion equation. We will define the equation in its most generic, transient 3-dimensional, form and then have an argument `dim` that can reduce it to lower dimensional forms.\n",
        "$$\\frac{\\partial T}{\\partial t}= \\nabla\\cdot \\left( D \\nabla T \\right) + Q$$\n",
        "\n",
        "Let's start defining the equation by inhereting from the `PDE` class. We will create the initialization method for this class that defines the equation(s) of interest. We will be defining the diffusion equation using the source(`Q`), diffusivity(`D`), symbol for diffusion(`T`). If `D` or `Q` is given as a string we will convert it to functional form. This will allow us to solve problems with spatially/temporally varying properties."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkhYKwH4uRH3"
      },
      "source": [
        "```python\n",
        "class Diffusion(PDE):\n",
        "    name = \"Diffusion\"\n",
        "\n",
        "    def __init__(self, T=\"T\", D=\"D\", Q=0, dim=3, time=True):\n",
        "        # set params\n",
        "        self.T = T\n",
        "        self.dim = dim\n",
        "        self.time = time\n",
        "\n",
        "        # coordinates\n",
        "        x, y, z = Symbol(\"x\"), Symbol(\"y\"), Symbol(\"z\")\n",
        "\n",
        "        # time\n",
        "        t = Symbol(\"t\")\n",
        "\n",
        "        # make input variables\n",
        "        input_variables = {\"x\": x, \"y\": y, \"z\": z, \"t\": t}\n",
        "        if self.dim == 1:\n",
        "            input_variables.pop(\"y\")\n",
        "            input_variables.pop(\"z\")\n",
        "        elif self.dim == 2:\n",
        "            input_variables.pop(\"z\")\n",
        "        if not self.time:\n",
        "            input_variables.pop(\"t\")\n",
        "\n",
        "        # Temperature\n",
        "        assert type(T) == str, \"T needs to be string\"\n",
        "        T = Function(T)(*input_variables)\n",
        "\n",
        "        # Diffusivity\n",
        "        if type(D) is str:\n",
        "            D = Function(D)(*input_variables)\n",
        "        elif type(D) in [float, int]:\n",
        "            D = Number(D)\n",
        "\n",
        "        # Source\n",
        "        if type(Q) is str:\n",
        "            Q = Function(Q)(*input_variables)\n",
        "        elif type(Q) in [float, int]:\n",
        "            Q = Number(Q)\n",
        "\n",
        "        # set equations\n",
        "        self.equations = {}\n",
        "        self.equations[\"diffusion_\" + self.T] = (\n",
        "            T.diff(t)\n",
        "            - (D * T.diff(x)).diff(x)\n",
        "            - (D * T.diff(y)).diff(y)\n",
        "            - (D * T.diff(z)).diff(z)\n",
        "            - Q\n",
        "        )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlJRMpDfuRH4"
      },
      "source": [
        "First we defined the input variables $x, y, z$ and $t$ with Sympy symbols. Then we defined the functions for $T$, $D$ and $Q$ that are dependent on the input variables $(x, y, z, t)$. Using these we can write out our simple equation $T_t = \\nabla \\cdot (D \\nabla T) + Q$. We store this equation in the class by adding it to the dictionary of `equations`.\n",
        "\n",
        "Note that we moved all the terms of the PDE either to LHS or RHS. This way, while using the equations in the constraints, we\n",
        "can assign a custom source function to the `’diffusion_T’` key instead of 0 to add more source terms to our PDE.\n",
        "\n",
        "Great! We just wrote our own PDE in Modulus! Once you have understood the process to code a simple PDE, you can easily extend the procedure for different PDEs. You can also bundle multiple PDEs together in a same file by adding new keys to the equations dictionary. Below we show the code for the interface boundary condition where we need to maintain the field (dirichlet) and flux (neumann) continuity. *(More examples of coding your own PDE can be found in the [Modulus User Documentation](https://docs.nvidia.com/deeplearning/modulus/text/foundational/1d_wave_equation.html))*.\n",
        "\n",
        "**Note :** The field continuity condition is needed because we are solving for two different temperatures in the two bars."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWEzbRiNuRH5"
      },
      "source": [
        "```python\n",
        "class DiffusionInterface(PDE):\n",
        "    name = \"DiffusionInterface\"\n",
        "\n",
        "    def __init__(self, T_1, T_2, D_1, D_2, dim=3, time=True):\n",
        "        # set params\n",
        "        self.T_1 = T_1\n",
        "        self.T_2 = T_2\n",
        "        self.dim = dim\n",
        "        self.time = time\n",
        "\n",
        "        # coordinates\n",
        "        x, y, z = Symbol(\"x\"), Symbol(\"y\"), Symbol(\"z\")\n",
        "        normal_x, normal_y, normal_z = (\n",
        "            Symbol(\"normal_x\"),\n",
        "            Symbol(\"normal_y\"),\n",
        "            Symbol(\"normal_z\"),\n",
        "        )\n",
        "\n",
        "        # time\n",
        "        t = Symbol(\"t\")\n",
        "\n",
        "        # make input variables\n",
        "        input_variables = {\"x\": x, \"y\": y, \"z\": z, \"t\": t}\n",
        "        if self.dim == 1:\n",
        "            input_variables.pop(\"y\")\n",
        "            input_variables.pop(\"z\")\n",
        "        elif self.dim == 2:\n",
        "            input_variables.pop(\"z\")\n",
        "        if not self.time:\n",
        "            input_variables.pop(\"t\")\n",
        "\n",
        "        # Diffusivity\n",
        "        if type(D_1) is str:\n",
        "            D_1 = Function(D_1)(*input_variables)\n",
        "        elif type(D_1) in [float, int]:\n",
        "            D_1 = Number(D_1)\n",
        "        if type(D_2) is str:\n",
        "            D_2 = Function(D_2)(*input_variables)\n",
        "        elif type(D_2) in [float, int]:\n",
        "            D_2 = Number(D_2)\n",
        "\n",
        "        # variables to match the boundary conditions (example Temperature)\n",
        "        T_1 = Function(T_1)(*input_variables)\n",
        "        T_2 = Function(T_2)(*input_variables)\n",
        "\n",
        "        # set equations\n",
        "        self.equations = {}\n",
        "        self.equations[\"diffusion_interface_dirichlet_\" + self.T_1 + \"_\" + self.T_2] = (\n",
        "            T_1 - T_2\n",
        "        )\n",
        "        flux_1 = D_1 * (\n",
        "            normal_x * T_1.diff(x) + normal_y * T_1.diff(y) + normal_z * T_1.diff(z)\n",
        "        )\n",
        "        flux_2 = D_2 * (\n",
        "            normal_x * T_2.diff(x) + normal_y * T_2.diff(y) + normal_z * T_2.diff(z)\n",
        "        )\n",
        "        self.equations[\"diffusion_interface_neumann_\" + self.T_1 + \"_\" + self.T_2] = (\n",
        "            flux_1 - flux_2\n",
        "        )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkT0fz0xuRH6"
      },
      "source": [
        "## Creating PDE and Neural Network nodes\n",
        "\n",
        "The default `FullyConnectedArch` (selected by setting `cfg.arch.fully_connected` in the `cfg` argument of the `instantiate_arch()` function) can be used to create the neural network for the problem. The default `FullyConnectedArch` represents a 6 layer MLP (multi-layer perceptron) architecture with each layer containing 512 nodes and uses swish as the activation function. All these parameters are user configurable, and can be configured using Hydra config files. Once all the PDEs and architectures are defined, we can create a list of nodes to pass to different constraints we want to satisfy for this problem (i.e. equations residuals, boundary conditions, etc.).\n",
        "\n",
        "We will define all the code for the problem in the `run` function as shown below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxrcMU9FuRH6"
      },
      "source": [
        "```python\n",
        "@modulus.main(config_path=\"conf\", config_name=\"config\")\n",
        "def run(cfg: ModulusConfig) -> None:\n",
        "\n",
        "    # make list of nodes to unroll graph on\n",
        "    diff_u1 = Diffusion(T=\"u_1\", D=D1, dim=1, time=False)\n",
        "    diff_u2 = Diffusion(T=\"u_2\", D=D2, dim=1, time=False)\n",
        "    diff_in = DiffusionInterface(\"u_1\", \"u_2\", D1, D2, dim=1, time=False)\n",
        "\n",
        "    diff_net_u_1 = instantiate_arch(\n",
        "        input_keys=[Key(\"x\")],\n",
        "        output_keys=[Key(\"u_1\")],\n",
        "        cfg=cfg.arch.fully_connected,\n",
        "    )\n",
        "    diff_net_u_2 = instantiate_arch(\n",
        "        input_keys=[Key(\"x\")],\n",
        "        output_keys=[Key(\"u_2\")],\n",
        "        cfg=cfg.arch.fully_connected,\n",
        "    )\n",
        "\n",
        "    nodes = (\n",
        "        diff_u1.make_nodes()\n",
        "        + diff_u2.make_nodes()\n",
        "        + diff_in.make_nodes()\n",
        "        + [diff_net_u_1.make_node(name=\"u1_network\", jit=cfg.jit)]\n",
        "        + [diff_net_u_2.make_node(name=\"u2_network\", jit=cfg.jit)]\n",
        "    )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqXddG8puRH6"
      },
      "source": [
        "### A quick note on Hydra to configure Modulus\n",
        "\n",
        "At the heart of the using Modulus are Hydra config files, the one for this example are found in the code shown below. Using hydra allows a highly customizable but user friendly method for configuring the majority of Modulus’ features. More information can be found in [Modulus Configuration](https://docs.nvidia.com/deeplearning/modulus/text/features/configuration.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcpCGdVkuRH7"
      },
      "source": [
        "```yaml\n",
        "defaults :\n",
        "  - modulus_default\n",
        "  - arch:\n",
        "      - fully_connected\n",
        "  - scheduler: tf_exponential_lr\n",
        "  - optimizer: adam\n",
        "  - loss: sum\n",
        "  - _self_\n",
        "\n",
        "arch:\n",
        "    fully_connected:\n",
        "        layer_size: 256\n",
        "\n",
        "save_filetypes : \"vtk,npz\"\n",
        "\n",
        "scheduler:\n",
        "  decay_rate: 0.95\n",
        "  decay_steps: 100\n",
        "\n",
        "optimizer:\n",
        "  lr : 1e-4\n",
        "\n",
        "training:\n",
        "  rec_results_freq: 1000\n",
        "  max_steps : 5000\n",
        "\n",
        "batch_size:\n",
        "  rhs: 2\n",
        "  lhs: 2\n",
        "  interface: 2\n",
        "  interior_u1: 200\n",
        "  interior_u2: 200\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crx0ZU8PuRH7"
      },
      "source": [
        "## Setting up the Domain: Assigning the boundary and PDE constraints\n",
        "\n",
        "As described earlier, we need to define a domain for training our neural network. The `Domain` and the configs are passed as inputs when using the `Solver` class. Apart from constraints, you can add various other utilites to the `Domain` such as monitors, validation data, points to do inference on, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIYgz7BRuRH7"
      },
      "source": [
        "```python\n",
        "    # make domain add constraints to the solver\n",
        "    domain = Domain()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMdAPDnVuRH8"
      },
      "source": [
        "Now let’s look into adding constraints to this domain. This can be thought of as adding specific constraints to the neural network optimization. For this physics-driven problem, these constraints are the boundary conditions and equation residuals. The goal is to satisfy the boundary conditions exactly, and ideally have the PDE residuals to go 0. These constraints can be specified within Modulus using classes like `PointwiseBoundaryConstrant` and `PointwiseInteriorConstraint`. A L2 loss (defult and can be modified) is then constructed from these constraints which is used by the optimizer to minimize on. Specifying the constraints in this fashion is called soft-constraints.  \n",
        "\n",
        "$$L = L_{BC} + L_{Residual}$$\n",
        "\n",
        "**Boundary constraints:** For generating a boundary condition, we need to sample the points on the required boundary/surface of the geometry, specify the nodes we would like to unroll/evaluate on these points and then assign them the desired values.\n",
        "\n",
        "A boundary can be sampled using `PointwiseBoundaryConstraint` class. This will sample the entire boundary of the geometry we specify in the `geometry` argument, in this case, both the endpoints of the 1d line. A particular boundary of the geometry can be sub-sampled by using a particular criterion using the `criteria` parameter. For example, to sample the left end of `L1`, `criteria` is set to `Eq(x, 0)`.\n",
        "\n",
        "The desired values for the boundary condition are listed as a dictionary in `outvar` argument. For this problem,  we have `'u_1':0` at $x=0$ and `'u_2':100` at $x=2$. At $x=1$, we have the interface condition `'diffusion_interface_dirichlet_u_1_u_2':0` and `'diffusion_interface_neumann_u_1_u_2':0` that we defined earlier (i.e. $U_1=U_2$ and $D_1\\frac{dU_1}{dx}=D_2\\frac{dU_2}{dx}$). These dictionaries are then used when unrolling the computational graph (specified using the `nodes` argument) for training.\n",
        "\n",
        "The number of points to sample on each boundary are specified using the `batch_size` parameter.\n",
        "\n",
        "**Equations to solve:** The Diffusion PDE we defined is enforced on all the points in the\n",
        "interior of both the bars, `L1` and `L2`. We will use `PointwiseInteriorConstraint` class to sample points in the interior of the geometry. Again, the appropriate geometry is specified in the `geometry` argument; the equations to solve are specified as a dictionary input to `outvar` argument. These dictionaries are then used when unrolling the computational graph (specified using the `nodes` argument) for training.\n",
        "\n",
        "For this problem we have the `'diffusion_u_1':0` and `'diffusion_u_2':0` for bars `L1` and `L2` respectively. The parameter `bounds`, determines the range for sampling the values for variables $x$ and $y$. The optional `lambda` parameter can be used to determine the weights for different losses. In this problem, we weight the loss on each point equally, and hence it is not used (defaults to 1) in this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wIxIa5VuRH8"
      },
      "source": [
        "```python\n",
        "    # sympy variables\n",
        "    x = Symbol(\"x\")\n",
        "\n",
        "    # right hand side (x = 2) Pt c\n",
        "    rhs = PointwiseBoundaryConstraint(\n",
        "        nodes=nodes,\n",
        "        geometry=L2,\n",
        "        outvar={\"u_2\": Tc},\n",
        "        batch_size=cfg.batch_size.rhs,\n",
        "        criteria=Eq(x, 2),\n",
        "    )\n",
        "    domain.add_constraint(rhs, \"right_hand_side\")\n",
        "\n",
        "    # left hand side (x = 0) Pt a\n",
        "    lhs = PointwiseBoundaryConstraint(\n",
        "        nodes=nodes,\n",
        "        geometry=L1,\n",
        "        outvar={\"u_1\": Ta},\n",
        "        batch_size=cfg.batch_size.lhs,\n",
        "        criteria=Eq(x, 0),\n",
        "    )\n",
        "    domain.add_constraint(lhs, \"left_hand_side\")\n",
        "\n",
        "    # interface 1-2\n",
        "    interface = PointwiseBoundaryConstraint(\n",
        "        nodes=nodes,\n",
        "        geometry=L1,\n",
        "        outvar={\n",
        "            \"diffusion_interface_dirichlet_u_1_u_2\": 0,\n",
        "            \"diffusion_interface_neumann_u_1_u_2\": 0,\n",
        "        },\n",
        "        batch_size=cfg.batch_size.interface,\n",
        "        criteria=Eq(x, 1),\n",
        "    )\n",
        "    domain.add_constraint(interface, \"interface\")\n",
        "\n",
        "    # interior 1\n",
        "    interior_u1 = PointwiseInteriorConstraint(\n",
        "        nodes=nodes,\n",
        "        geometry=L1,\n",
        "        outvar={\"diffusion_u_1\": 0},\n",
        "        bounds={x: (0, 1)},\n",
        "        batch_size=cfg.batch_size.interior_u1,\n",
        "    )\n",
        "    domain.add_constraint(interior_u1, \"interior_u1\")\n",
        "\n",
        "    # interior 2\n",
        "    interior_u2 = PointwiseInteriorConstraint(\n",
        "        nodes=nodes,\n",
        "        geometry=L2,\n",
        "        outvar={\"diffusion_u_2\": 0},\n",
        "        bounds={x: (1, 2)},\n",
        "        batch_size=cfg.batch_size.interior_u2,\n",
        "    )\n",
        "    domain.add_constraint(interior_u2, \"interior_u2\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbzQOiASuRH8"
      },
      "source": [
        "## Adding Validation Data\n",
        "\n",
        "For this 1d bar problem where the conductivity is constant in each bar, the temperature varies linearly with position inside the solid. The analytical solution can then be given as:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "U_1 = xT_b + (1-x)T_a, && \\text{when } 0 \\leq x \\leq 1 \\\\\n",
        "U_2 = (x-1)T_c + (2-x)T_b, && \\text{when } 1 \\leq x \\leq 2 \\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where,\n",
        "$$\n",
        "\\begin{align}\n",
        "T_a = U_1|_{x=0}, && T_c = U_2|_{x=2}, && \\frac{\\left(T_c + \\left( D_1/D_2 \\right)T_a \\right)}{1+ \\left( D_1/D_2 \\right)}\\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Now let's create the validators. The validation data is added to the domain using the `PointwiseValidator` class. We use numpy to solve for the `u_1` and `u_2` based on the analytical expressions we showed above. The dictionary of generated numpy arrays (`invar_numpy` and `outvar_numpy`) for input and output variables and the appropriate nodes are used in the definition of the constructor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK6LgMiVuRH9"
      },
      "source": [
        "```python\n",
        "    # validation data\n",
        "    x = np.expand_dims(np.linspace(0, 1, 100), axis=-1)\n",
        "    u_1 = x * Tb + (1 - x) * Ta\n",
        "    invar_numpy = {\"x\": x}\n",
        "    outvar_numpy = {\"u_1\": u_1}\n",
        "    val = PointwiseValidator(nodes=nodes,invar=invar_numpy, true_outvar=outvar_numpy)\n",
        "    domain.add_validator(val, name=\"Val1\")\n",
        "\n",
        "    # make validation data line 2\n",
        "    x = np.expand_dims(np.linspace(1, 2, 100), axis=-1)\n",
        "    u_2 = (x - 1) * Tc + (2 - x) * Tb\n",
        "    invar_numpy = {\"x\": x}\n",
        "    outvar_numpy = {\"u_2\": u_2}\n",
        "    val = PointwiseValidator(nodes=nodes, invar=invar_numpy, true_outvar=outvar_numpy)\n",
        "    domain.add_validator(val, name=\"Val2\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69giZ8MLuRH9"
      },
      "source": [
        "## Adding Monitors\n",
        "\n",
        "Modulus library allows you to monitor desired quantities in Tensorboard as the simulation progresses and\n",
        "assess the convergence. A `PointwiseMonitor` can be used to create such an feature. Examples of such quantities can be point values of variables, surface averages, volume averages or any derived quantities that can be formed using the variables being solved.\n",
        "\n",
        "The variables are available as PyTorch tensors. We can perform tensor operations available in PyTorch to compute any desired derived quantity of our choice.\n",
        "\n",
        "In the code below, we create monitors for flux at the interface. The variable `u_1__x` represents the derivative of `u_1` in x-direction (two underscores (`__`) and the variable (`x`)). The same notation is used while handling other derivatives using the Modulus library. (eg. a neumann boundary condition of $\\frac{dU_1}{dx}=0$ can be assigned as `'u_1__x':0` in the train domain for solving the same problem with a adiabatic/fixed flux boundary condition).\n",
        "\n",
        "The points to sample can be selected in a similar way as we did for specifying some of the interior constraints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA0gdeW8uRH-"
      },
      "source": [
        "```python\n",
        "    # make monitors\n",
        "    invar_numpy = {\"x\": [[1.0]]}\n",
        "    monitor = PointwiseMonitor(\n",
        "        invar_numpy,\n",
        "        output_names=[\"u_1__x\"],\n",
        "        metrics={\"flux_u1\": lambda var: torch.mean(var[\"u_1__x\"])},\n",
        "        nodes=nodes,\n",
        "        requires_grad=True,\n",
        "    )\n",
        "    domain.add_monitor(monitor)\n",
        "\n",
        "    monitor = PointwiseMonitor(\n",
        "        invar_numpy,\n",
        "        output_names=[\"u_2__x\"],\n",
        "        metrics={\"flux_u2\": lambda var: torch.mean(var[\"u_2__x\"])},\n",
        "        nodes=nodes,\n",
        "        requires_grad=True,\n",
        "    )\n",
        "    domain.add_monitor(monitor)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYjOQfZbuRH-"
      },
      "source": [
        "## Putting everything together: Solver and training\n",
        "\n",
        "We can create a solver by using the domain we just created along with the other configurations that define the optimizer choices, settings (i.e. conf) using Modulus’ `Solver` class. The solver can then be executed using the `solve` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS_ThOLJuRH_"
      },
      "source": [
        "```python\n",
        "    # make solver\n",
        "    slv = Solver(cfg, domain)\n",
        "\n",
        "    # start solver\n",
        "    slv.solve()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFN36jQRuRH_"
      },
      "source": [
        "Awesome! We have just completed the file set up for the problem using the Modulus library. We are now ready to solve the PDEs using Neural Networks!\n",
        "\n",
        "Before we can start training, we can make use of Tensorboard for visualizing the loss values and convergence of several other monitors we just created. This can be done inside the jupyter framework by selecting the directory in which the checkpoint will be stored by clicking on the small checkbox next to it. The option to launch a Tensorboard then shows up in that directory.\n",
        "\n",
        "<img src=\"https://github.com/robertopsouto/gpubootcamp/blob/colab/hpc_ai/PINN/English/python/jupyter_notebook/diffusion_1d/image_tensorboard.png?raw=1\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
        "\n",
        "Also, as Modulus uses Hydra for config management, this causes issues when the code is directly executed through the jupyter notebook. Hence, we will save the code in form of a python script and execute that script inside the jupyter cell. This example is already saved for you and the code block below executes that script [`diffusion_bar.py`](../../source_code/diffusion_1d/diffusion_bar.py). You are encouraged to open the script and go through the code once before executing. Also, feel free to edit the parameters in the [`config.yaml`](../../source_code/diffusion_1d/conf/) file of the model and see its effect on the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "v-JlBTASuRIA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"RANK\"]=\"0\"\n",
        "os.environ[\"WORLD_SIZE\"]=\"1\"\n",
        "os.environ[\"MASTER_ADDR\"]=\"localhost\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "git clone https://github.com/robertopsouto/gpubootcamp.git\n",
        "#cd gpubootcamp/hpc_ai/PINN/English/python/jupyter_notebook/diffusion_1d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xT9O28-Rxbkv",
        "outputId": "785d19d4-dc77-40e3-f624-009aabd3f053"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'gpubootcamp'...\n",
            "Updating files:  93% (1704/1823)\rUpdating files:  94% (1714/1823)\rUpdating files:  95% (1732/1823)\rUpdating files:  96% (1751/1823)\rUpdating files:  97% (1769/1823)\rUpdating files:  98% (1787/1823)\rUpdating files:  99% (1805/1823)\rUpdating files: 100% (1823/1823)\rUpdating files: 100% (1823/1823), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gpubootcamp/hpc_ai/PINN/English/python/jupyter_notebook/diffusion_1d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDaOMcCxygBz",
        "outputId": "369390fa-2f2b-4021-8579-3d3d02bb1f19"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gpubootcamp/hpc_ai/PINN/English/python/jupyter_notebook/diffusion_1d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vn-7uB2WuRID",
        "outputId": "c977f064-2203-42e6-cf9f-a3a75a4dff74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/gpubootcamp/hpc_ai/PINN/English/python/jupyter_notebook/diffusion_1d/../../source_code/diffusion_1d/diffusion_bar.py\", line 6, in <module>\n",
            "    from modulus.hydra import instantiate_arch , ModulusConfig\n",
            "ModuleNotFoundError: No module named 'modulus.hydra'\n"
          ]
        }
      ],
      "source": [
        "!python3 ../../source_code/diffusion_1d/diffusion_bar.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ9LuLH_uRID"
      },
      "source": [
        "## Visualizing the solution\n",
        "\n",
        "Modulus saves the data in several formats (including .vtp, .vtk and .npz). The .npz arrays can be plotted to visualize the output of the simulation. The .npz files that are created are found in the `outputs/` directory.\n",
        "\n",
        "Now let's plot the temperature along the bar for the analytical and the neural network solution. A sample script to plot the results is shown below. If the training is complete, you should get the results like shown below. As we can see, our neural network solution and the analytical solution match almost exactly for this diffusion problem.\n",
        "\n",
        "<img src=\"https://github.com/robertopsouto/gpubootcamp/blob/colab/hpc_ai/PINN/English/python/jupyter_notebook/diffusion_1d/image_diffusion_problem_bootcamp.png?raw=1\" alt=\"Drawing\" style=\"width: 500px;\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "4snqzK0HuRIE",
        "outputId": "13d26f5e-0da9-4cdb-a3fa-57a33d4bf363"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1859cc8bd164>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnetwork_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./outputs/diffusion_bar/validators/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdata_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"Val1.npz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdata_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"Val2.npz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdata_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marr_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './outputs/diffusion_bar/validators/Val1.npz'"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "import sys\n",
        "!{sys.executable} -m pip install ipympl\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "network_dir = \"./outputs/diffusion_bar/validators/\"\n",
        "data_1 = np.load(network_dir + \"Val1.npz\", allow_pickle=True)\n",
        "data_2 = np.load(network_dir + \"Val2.npz\", allow_pickle=True)\n",
        "data_1 = np.atleast_1d(data_1.f.arr_0)[0]\n",
        "data_2 = np.atleast_1d(data_2.f.arr_0)[0]\n",
        "\n",
        "plt.plot(data_1[\"x\"][:, 0], data_1[\"pred_u_1\"][:, 0], \"--\", label=\"u_1_pred\")\n",
        "plt.plot(data_2[\"x\"][:, 0], data_2[\"pred_u_2\"][:, 0], \"--\", label=\"u_2_pred\")\n",
        "plt.plot(data_1[\"x\"][:, 0], data_1[\"true_u_1\"][:, 0], label=\"u_1_true\")\n",
        "plt.plot(data_2[\"x\"][:, 0], data_2[\"true_u_2\"][:, 0], label=\"u_2_true\")\n",
        "\n",
        "plt.legend()\n",
        "plt.savefig(\"image_diffusion_problem_bootcamp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk6NAKdEuRIE"
      },
      "source": [
        "# Parameterizing the PDE\n",
        "\n",
        "As we discussed in the introductory notebook, one important advantage of a PINN solver over traditional numerical methods is its ability to solve parameterized geometries and PDEs. This was initially proposed in the [paper](https://arxiv.org/abs/1906.02382) published by Sun et al. This allows us significant computational advantage, as one can now use PINNs to solve for multiple designs/cases in a single training. Once the training is complete, it is possible to run inference on several geometry/physical parameter combinations as a post-processing step, without solving the forward problem again.\n",
        "\n",
        "To demonstrate the concept, we will train the same 1d diffusion problem, but now by parameterizing the conductivity of the first bar in the range (5, 25). Once the training is complete, we can obtain the results for any conductivity value in that range saving us the time to train multiple models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOkk7cHyuRIE"
      },
      "source": [
        "## Case Setup\n",
        "\n",
        "The definition of equations remain the same for this part. Since earlier while defining the equations, we already defined the constants and coefficients of the PDE to be either numerical values or strings, this will allow us to paramterize `D1` by passing it as a string while calling the equations and making the neural network. Now let's start by creating the paramterized train domain. We will skip the parts that are common to the previous section and only discuss the changes. The complete script can be referred in [`diffusion_bar_parameterized.py`](../../source_code/diffusion_1d/diffusion_bar_parameterized.py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkMicQZUuRIF"
      },
      "source": [
        "## Adding Parameterized PDE and Neural Network nodes\n",
        "\n",
        "Before starting out to create the nodes and domain, we will create the symbolic variable for the $D_1$ and also specify the range of variation for the variable. While the simulation runs, we will validate it against the same diffusion coefficient that we solved earlier i.e. $D_1=10$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5ioHXrCuRIF"
      },
      "source": [
        "```python\n",
        "# params for domain\n",
        "L1 = Line1D(0, 1)\n",
        "L2 = Line1D(1, 2)\n",
        "\n",
        "D1 = Symbol(\"D1\")\n",
        "D1_range = {D1: (5, 25)}\n",
        "D1_validation = 1e1\n",
        "\n",
        "D2 = 1e-1\n",
        "\n",
        "Tc = 100\n",
        "Ta = 0\n",
        "Tb = (Tc + (D1 / D2) * Ta) / (1 + (D1 / D2))\n",
        "Tb_validation = float(Tb.evalf(subs={D1: 1e1}))\n",
        "\n",
        "print(Ta)\n",
        "print(Tb)\n",
        "print(Tc)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQrTer3quRIF"
      },
      "source": [
        "For training the parameterized model, we will have the symbolic parameters we just defined as inputs to both the neural networks in `diff_net_u_1` and `diff_net_u_2` viz. `'D1'` along with the usual x coordinate. The outputs remain the same as what we would have for any other non-parameterized simulation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx3Veey7uRIM"
      },
      "source": [
        "```python\n",
        "@modulus.main(config_path=\"conf\", config_name=\"config_param\")\n",
        "def run(cfg: ModulusConfig) -> None:\n",
        "    # make list of nodes to unroll graph on\n",
        "    diff_u1 = Diffusion(T=\"u_1\", D=\"D1\", dim=1, time=False)\n",
        "    diff_u2 = Diffusion(T=\"u_2\", D=D2, dim=1, time=False)\n",
        "    diff_in = DiffusionInterface(\"u_1\", \"u_2\", \"D1\", D2, dim=1, time=False)\n",
        "\n",
        "    diff_net_u_1 = instantiate_arch(\n",
        "        input_keys=[Key(\"x\"), Key(\"D1\")],\n",
        "        output_keys=[Key(\"u_1\")],\n",
        "        cfg=cfg.arch.fully_connected,\n",
        "    )\n",
        "    diff_net_u_2 = instantiate_arch(\n",
        "        input_keys=[Key(\"x\"), Key(\"D1\")],\n",
        "        output_keys=[Key(\"u_2\")],\n",
        "        cfg=cfg.arch.fully_connected,\n",
        "    )\n",
        "\n",
        "    nodes = (\n",
        "        diff_u1.make_nodes()\n",
        "        + diff_u2.make_nodes()\n",
        "        + diff_in.make_nodes()\n",
        "        + [diff_net_u_1.make_node(name=\"u1_network\", jit=cfg.jit)]\n",
        "        + [diff_net_u_2.make_node(name=\"u2_network\", jit=cfg.jit)]\n",
        "    )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEFxXdPIuRIN"
      },
      "source": [
        "## Adding Parameterized boundary and PDE Constraints\n",
        "\n",
        "This part of the code is very similar to the non-parameterized version. The sybolic variables and the ranges that we described earlier need to be inputted to the `param_ranges` attribute of each boundary and internal constraints (`PointwiseBoundaryConstraint` and `PointwiseInteriorConstraint`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA5CNONmuRIN"
      },
      "source": [
        "```python\n",
        "    # make domain add constraints to the solver\n",
        "    domain = Domain()\n",
        "\n",
        "    # sympy variables\n",
        "    x = Symbol(\"x\")\n",
        "\n",
        "    # right hand side (x = 2) Pt c\n",
        "    rhs = PointwiseBoundaryConstraint(\n",
        "        nodes=nodes,\n",
        "        geometry=L2,\n",
        "        outvar={\"u_2\": Tc},\n",
        "        batch_size=cfg.batch_size.rhs,\n",
        "        criteria=Eq(x, 2),\n",
        "        parameterization=Parameterization(D1_range),\n",
        "    )\n",
        "    domain.add_constraint(rhs, \"right_hand_side\")\n",
        "\n",
        "    # left hand side (x = 0) Pt a\n",
        "    lhs = PointwiseBoundaryConstraint(\n",
        "        nodes=nodes,\n",
        "        geometry=L1,\n",
        "        outvar={\"u_1\": Ta},\n",
        "        batch_size=cfg.batch_size.lhs,\n",
        "        criteria=Eq(x, 0),\n",
        "        parameterization=Parameterization(D1_range),\n",
        "    )\n",
        "    domain.add_constraint(lhs, \"left_hand_side\")\n",
        "\n",
        "    # interface 1-2\n",
        "    interface = PointwiseBoundaryConstraint(\n",
        "        nodes=nodes,\n",
        "        geometry=L1,\n",
        "        outvar={\n",
        "            \"diffusion_interface_dirichlet_u_1_u_2\": 0,\n",
        "            \"diffusion_interface_neumann_u_1_u_2\": 0,\n",
        "        },\n",
        "        batch_size=cfg.batch_size.interface,\n",
        "        criteria=Eq(x, 1),\n",
        "        parameterization=Parameterization(D1_range),\n",
        "    )\n",
        "    domain.add_constraint(interface, \"interface\")\n",
        "\n",
        "    # interior 1\n",
        "    interior_u1 = PointwiseInteriorConstraint(\n",
        "        nodes=nodes,\n",
        "        geometry=L1,\n",
        "        outvar={\"diffusion_u_1\": 0},\n",
        "        bounds={x: (0, 1)},\n",
        "        batch_size=cfg.batch_size.interior_u1,\n",
        "        parameterization=Parameterization(D1_range),\n",
        "    )\n",
        "    domain.add_constraint(interior_u1, \"interior_u1\")\n",
        "\n",
        "    # interior 2\n",
        "    interior_u2 = PointwiseInteriorConstraint(\n",
        "        nodes=nodes,\n",
        "        geometry=L2,\n",
        "        outvar={\"diffusion_u_2\": 0},\n",
        "        bounds={x: (1, 2)},\n",
        "        batch_size=cfg.batch_size.interior_u2,\n",
        "        parameterization=Parameterization(D1_range),\n",
        "    )\n",
        "    domain.add_constraint(interior_u2, \"interior_u2\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0JmVqfbuRIO"
      },
      "source": [
        "## Adding Validators and Monitors\n",
        "\n",
        "The process to create these domains is again similar to the previous section. For validation data, we need to create an additional key for the string `'D1'` in the `invar_numpy`. The value for this key can be in the range we specified earlier and which we would like to validate against. It is possible to create multiple validators if required, eg. different $D_1$ values. For monitor domain, similar `invar_numpy` is generated that has both the `'x'` and `'D1'` keys and appropriate arrays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxGE3sRfuRIO"
      },
      "source": [
        "```python\n",
        "    # validation data\n",
        "    x = np.expand_dims(np.linspace(0, 1, 100), axis=-1)\n",
        "    u_1 = x * Tb_validation + (1 - x) * Ta\n",
        "    invar_numpy = {\"x\": x}\n",
        "    invar_numpy.update({\"D1\": np.full_like(invar_numpy[\"x\"], D1_validation)})\n",
        "    outvar_numpy = {\"u_1\": u_1}\n",
        "    val = PointwiseValidator(nodes=nodes,invar=invar_numpy, true_outvar=outvar_numpy)\n",
        "    domain.add_validator(val, name=\"Val1\")\n",
        "    \n",
        "    # make validation data line 2\n",
        "    x = np.expand_dims(np.linspace(1, 2, 100), axis=-1)\n",
        "    u_2 = (x - 1) * Tc + (2 - x) * Tb_validation\n",
        "    invar_numpy = {\"x\": x}\n",
        "    invar_numpy.update({\"D1\": np.full_like(invar_numpy[\"x\"], D1_validation)})\n",
        "    outvar_numpy = {\"u_2\": u_2}\n",
        "    val = PointwiseValidator(nodes=nodes, invar=invar_numpy, true_outvar=outvar_numpy)\n",
        "    domain.add_validator(val, name=\"Val2\")\n",
        "    \n",
        "    # make monitors\n",
        "    invar_numpy = {\"x\": [[1.0]], \"D1\": [[D1_validation]]}\n",
        "    monitor = PointwiseMonitor(\n",
        "        invar_numpy,\n",
        "        output_names=[\"u_1__x\"],\n",
        "        metrics={\"flux_u1\": lambda var: torch.mean(var[\"u_1__x\"])},\n",
        "        nodes=nodes,\n",
        "        requires_grad=True,\n",
        "    )\n",
        "    domain.add_monitor(monitor)\n",
        "\n",
        "    monitor = PointwiseMonitor(\n",
        "        invar_numpy,\n",
        "        output_names=[\"u_2__x\"],\n",
        "        metrics={\"flux_u2\": lambda var: torch.mean(var[\"u_2__x\"])},\n",
        "        nodes=nodes,\n",
        "        requires_grad=True,\n",
        "    )\n",
        "    domain.add_monitor(monitor)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFhSjqCzuRIP"
      },
      "source": [
        "## Solver and Training\n",
        "\n",
        "Now that we have the definitions for the various constraints and domains complete, we can form the solver and run the problem similar to before. The code to do the same can be found below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_GEiEQDuRIQ"
      },
      "source": [
        "```python\n",
        "    # make solver\n",
        "    slv = Solver(cfg, domain)\n",
        "\n",
        "    # start solver\n",
        "    slv.solve()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AlrUpTauRIQ"
      },
      "outputs": [],
      "source": [
        "!python3 ../../source_code/diffusion_1d/diffusion_bar_parameterized.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDpYiy1ruRIR"
      },
      "source": [
        "## Visualizing the solution\n",
        "\n",
        "The .npz arrays can be plotted similar to previous section to visualize the output of the simulation. You can see that we get the same answer as the analytical solution. You can try to run the problem in `eval` mode by chaning the validation data and see how it performs for the other `D1` values as well. To run the model in evaluation mode (i.e. without training), you just need to [modify the config file](https://docs.nvidia.com/deeplearning/modulus/text/features/configuration.html#run-modes).  \n",
        "\n",
        "<img src=\"https://github.com/robertopsouto/gpubootcamp/blob/colab/hpc_ai/PINN/English/python/jupyter_notebook/diffusion_1d/image_diffusion_problem_bootcamp_parameterized.png?raw=1\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
        "\n",
        "You can see that at a fractional increase in computational time, we solved the PDE for $D_1$ ranging from (5, 25). This concept can easily be extended to more complicated problems and this ability of solving parameterized problems comes very handy during desing optimization and exploring the design space. For more examples of solving parameterized problems, please refer to [Modulus User Documentation](https://docs.nvidia.com/deeplearning/modulus/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-5CeEKwuRIS"
      },
      "source": [
        "# Licensing\n",
        "This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}